{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe588324",
   "metadata": {},
   "source": [
    "Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d415539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the folder containing your script\n",
    "script_dir = os.getcwd() # \n",
    "\n",
    "# Build the path relative to that folder\n",
    "address2 = os.path.join(script_dir, \"Raw Images\", \"cam0_capture_2_20250828_054449.jpg\")\n",
    "address1 = os.path.join(script_dir, \"Raw Images\", \"cam1_capture_2_20250828_054449.jpg\")\n",
    "\n",
    "img1 = cv2.imread(address1)\n",
    "img2 = cv2.imread(address2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc02ba8",
   "metadata": {},
   "source": [
    "2 Frames Alignment Function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c39c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned image to: Aligned and Blended images\\cam0_capture_2_20250828_054449_aligned.jpg\n",
      "Saved blended image to: Aligned and Blended images\\cam0_capture_2_20250828_054449_blended.jpg\n"
     ]
    }
   ],
   "source": [
    "# This script aligns two images using feature-based methods.\n",
    "# It uses ORB feature detection and matching to compute a homography matrix,   \n",
    "# which is then used to warp one image to align with the other.\n",
    "# The aligned image is then tested against the original image to ensure alignment.\n",
    "\n",
    "\n",
    "## 03.09.2025 22:16: Script doesn't align properly lowlight images, and saves in the wrong folder\n",
    "\n",
    "\n",
    "def align_images_feature_based(img1, img2):\n",
    "    \"\"\"\n",
    "    Align img2 to img1 using ORB feature matching + homography.\n",
    "    Handles both grayscale and color images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure grayscale conversion only if needed\n",
    "    if len(img1.shape) == 3:  # color\n",
    "        gray1 = cv2.equalizeHist(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY))\n",
    "    else:  # already grayscale\n",
    "        gray1 = cv2.equalizeHist(img1)\n",
    "\n",
    "    if len(img2.shape) == 3:  # color\n",
    "        gray2 = cv2.equalizeHist(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY))\n",
    "    else:  # already grayscale\n",
    "        gray2 = cv2.equalizeHist(img2)\n",
    "\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    orb = cv2.ORB_create(5000)\n",
    "    kp1, des1 = orb.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    if len(matches) < 4:\n",
    "        raise ValueError(\"Not enough matches to compute homography.\")\n",
    "\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate the homography matrix\n",
    "    M, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp img2 to align with img1 (keep same size as img1)\n",
    "    aligned = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]))\n",
    "    \n",
    "    inliers = mask.sum() if mask is not None else 0\n",
    "\n",
    "    return aligned, inliers\n",
    "\n",
    "\n",
    "def align_images_with_mirror_check(img1, img2):\n",
    "    best_aligned = None\n",
    "    best_inliers = -1\n",
    "\n",
    "    # Try normal alignment\n",
    "    # try:\n",
    "    #     aligned, inliers = align_images_feature_based(img1, img2)\n",
    "    #     if inliers > best_inliers:\n",
    "    #         best_inliers = inliers\n",
    "    #         best_aligned = aligned\n",
    "    # except Exception as e:\n",
    "    #     print(\"Normal alignment failed:\", e)\n",
    "\n",
    "    # Try mirrored alignment\n",
    "    img2_flipped = cv2.flip(img2, 1)\n",
    "    try:\n",
    "        aligned, inliers = align_images_feature_based(img1, img2_flipped)\n",
    "        if inliers > best_inliers:\n",
    "            best_inliers = inliers\n",
    "            best_aligned = aligned\n",
    "    except Exception as e:\n",
    "        print(\"Mirrored alignment failed:\", e)\n",
    "\n",
    "    # if best_aligned is None:\n",
    "        raise ValueError(\"Failed to align both normal and mirrored cases.\")\n",
    "\n",
    "    return best_aligned\n",
    "\n",
    "\n",
    "def overlay_images_rgb(img1, img2_aligned, alpha=0.5):\n",
    "    # Resize img2_aligned to match img1 if needed\n",
    "    if img1.shape != img2_aligned.shape:\n",
    "        img2_aligned = cv2.resize(img2_aligned, (img1.shape[1], img1.shape[0]))\n",
    "\n",
    "    # Blend images using alpha\n",
    "    blended = cv2.addWeighted(img1, alpha, img2_aligned, 1 - alpha, 0)\n",
    "    return blended\n",
    "\n",
    "\n",
    "\n",
    "# Align img2 to img1\n",
    "aligned = align_images_with_mirror_check(img1, img2)\n",
    "\n",
    "# Overlay for visual verification\n",
    "blended = overlay_images_rgb(img1, aligned)\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "output_folder = \"Aligned and Blended images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Split into folder, base filename, and extension\n",
    "base2 = os.path.splitext(os.path.basename(address2))[0]   # cam0_capture_1_20250828_051554\n",
    "ext2 = os.path.splitext(address2)[1]                      # .jpg\n",
    " \n",
    "# Build new paths\n",
    "aligned_filename = os.path.join(output_folder, f\"{base2}_aligned{ext2}\")\n",
    "blended_filename = os.path.join(output_folder, f\"{base2}_blended{ext2}\")\n",
    "\n",
    "cv2.imwrite(aligned_filename, aligned)\n",
    "cv2.imwrite(blended_filename, blended)\n",
    "\n",
    "print(f\"Saved aligned image to: {aligned_filename}\")\n",
    "print(f\"Saved blended image to: {blended_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2b0a8",
   "metadata": {},
   "source": [
    "2 Frames Focus Assessment Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21da5131",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     variance = laplacian.var()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m variance\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m loss1 = \u001b[43mfocus_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m loss2 = focus_loss(img2)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFocus loss for img1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, img2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mfocus_loss\u001b[39m\u001b[34m(image)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfocus_loss\u001b[39m(image):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# If already grayscale, skip conversion\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m) == \u001b[32m2\u001b[39m:  \u001b[38;5;66;03m# single channel\u001b[39;00m\n\u001b[32m      4\u001b[39m         gray = image\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "def focus_loss(image):\n",
    "    # If already grayscale, skip conversion\n",
    "    if len(image.shape) == 2:  # single channel\n",
    "        gray = image\n",
    "    else:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "loss1 = focus_loss(img1)\n",
    "loss2 = focus_loss(img2)\n",
    "print(f\"Focus loss for img1: {loss1}, img2: {loss2}\")\n",
    "focus_threshold = 0.5  # Example threshold for focus quality\n",
    "if np.abs(loss1 - loss2) < focus_threshold:\n",
    "    print(\"The images are in focus.\")\n",
    "else:\n",
    "    print(\"The images are not in focus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89db54f",
   "metadata": {},
   "source": [
    "Video Alignment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db556f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP4 already exists: cam0 record.mp4\n",
      "MP4 already exists: cam1 record.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import cv2\n",
    "\n",
    "# ---------------------------\n",
    "# Helper function to convert & rotate raw H.264\n",
    "# ---------------------------\n",
    "def remux_and_rotate_h264(input_file, output_file, fps=25, rotation=None):\n",
    "    \"\"\"\n",
    "    Convert raw H.264 to MP4 with optional rotation (re-encoding required).\n",
    "    rotation: None, 90, 180, 270 (degrees clockwise)\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"MP4 already exists: {output_file}\")\n",
    "        return\n",
    "\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-framerate\", str(fps), \"-i\", input_file]\n",
    "\n",
    "    # Apply rotation with ffmpeg filters\n",
    "    if rotation == 90:\n",
    "        cmd += [\"-vf\", \"transpose=1\"]  # 90° clockwise\n",
    "    elif rotation == 180:\n",
    "        cmd += [\"-vf\", \"transpose=1,transpose=1\"]  # 180° (two 90° clockwise)\n",
    "    elif rotation == 270:\n",
    "        cmd += [\"-vf\", \"transpose=2\"]  # 90° counter-clockwise (270° clockwise)\n",
    "\n",
    "    # Re-encode with libx264 to apply rotation\n",
    "    cmd += [\"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", output_file]\n",
    "\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"Created MP4: {output_file}\")\n",
    "\n",
    "# ---------------------------\n",
    "# File paths\n",
    "# ---------------------------\n",
    "cam0_h264 = \"cam0 record.h264\"\n",
    "cam1_h264 = \"cam1 record.h264\"\n",
    "cam0_mp4 = \"cam0 record.mp4\"\n",
    "cam1_mp4 = \"cam1 record.mp4\"\n",
    "output_aligned = \"cam1 record_aligned.mp4\"\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Convert and rotate raw H.264 to MP4\n",
    "# ---------------------------\n",
    "remux_and_rotate_h264(cam0_h264, cam0_mp4, fps=25, rotation=180)\n",
    "remux_and_rotate_h264(cam1_h264, cam1_mp4, fps=25, rotation=90)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Open MP4s with OpenCV\n",
    "# ---------------------------\n",
    "cap0 = cv2.VideoCapture(cam0_mp4)  # reference\n",
    "cap1 = cv2.VideoCapture(cam1_mp4)  # to align\n",
    "\n",
    "ret0, frame_ref = cap0.read()\n",
    "ret1, frame_target = cap1.read()\n",
    "if not (ret0 and ret1):\n",
    "    raise ValueError(\"Failed to read the first frames from the videos.\")\n",
    "\n",
    "h, w = frame_ref.shape[:2]\n",
    "fps = cap0.get(cv2.CAP_PROP_FPS) or 25\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_aligned, fourcc, fps, (w, h))\n",
    "\n",
    "# Reset to first frame\n",
    "cap0.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "cap1.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Frame-by-frame alignment\n",
    "# ---------------------------\n",
    "while True:\n",
    "    ret0, frame_ref = cap0.read()\n",
    "    ret1, frame_target = cap1.read()\n",
    "    if not (ret0 and ret1):\n",
    "        break\n",
    "\n",
    "    # Align cam1 to cam0\n",
    "    aligned_frame = align_images_with_mirror_check(frame_ref, frame_target)\n",
    "    out.write(aligned_frame)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Cleanup\n",
    "# ---------------------------\n",
    "cap0.release()\n",
    "cap1.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Alignment complete. Output saved to:\", output_aligned)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
