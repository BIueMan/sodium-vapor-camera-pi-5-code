{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe588324",
   "metadata": {},
   "source": [
    "Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d415539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "# Path to the folder containing your script\n",
    "script_dir = os.getcwd() # \n",
    "\n",
    "# Build the path relative to that folder - RAW IMAGES FOLDER\n",
    "address2 = os.path.join(script_dir, \"Raw Images\", \"cam0_capture_20250828_063612.jpg\") #cam 0 is the camera behind the Bandpass filter, has less light, needs to be aligned to cam 1\n",
    "address1 = os.path.join(script_dir, \"Raw Images\", \"cam1_capture_20250828_063612.jpg\") #cam 1 is the camera behind the Notch filter, has more light and serves as reference\n",
    "\n",
    "# # Build the path relative to that folder - NO FILTER data FOLDER\n",
    "# address2 = os.path.join(script_dir, \"NO FILTER data\", \"cam0_me_lowlight.jpg\") #cam 0 is the camera behind the Bandpass filter, has less light, needs to be aligned to cam 1\n",
    "# address1 = os.path.join(script_dir, \"NO FILTER data\", \"cam1_me_lowlight.jpg\") #cam 1 is the camera behind the Notch filter, has more light and serves as reference\n",
    "\n",
    "# Build the path relative to that folder - RAW VIDEOS FOLDER\n",
    "vid_address2 = os.path.join(script_dir, \"Raw Videos\", \"cam0_capture_20250828_063612.h264\") #cam 0 is the camera behind the Bandpass filter, has less light, needs to be aligned to cam 1\n",
    "vid_address1 = os.path.join(script_dir, \"Raw Videos\", \"cam1_capture_20250828_063612.h264\") #cam 1 is the camera behind the Notch filter, has more light and serves as reference\n",
    "\n",
    "# # Build the path relative to that folder - RAW VIDEOS FOLDER\n",
    "# vid_address2 = os.path.join(script_dir, \"NO FILTER data\", \"cam0 record.h264\") #cam 0 is the camera behind the Bandpass filter, has less light, needs to be aligned to cam 1\n",
    "# vid_address1 = os.path.join(script_dir, \"NO FILTER data\", \"cam1 record.h264\") #cam 1 is the camera behind the Notch filter, has more light and serves as reference\n",
    "\n",
    "\n",
    "\n",
    "img1 = cv2.imread(address1)\n",
    "img2 = cv2.imread(address2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc02ba8",
   "metadata": {},
   "source": [
    "2 Frames Alignment Function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19c39c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned image to: Aligned and Blended images\\cam0_capture_2_20250828_054449_aligned.jpg\n",
      "Saved blended image to: Aligned and Blended images\\cam0_capture_2_20250828_054449_blended.jpg\n"
     ]
    }
   ],
   "source": [
    "# This script aligns two images using feature-based methods.\n",
    "# It uses ORB feature detection and matching to compute a homography matrix,   \n",
    "# which is then used to warp one image to align with the other.\n",
    "# The aligned image is then tested against the original image to ensure alignment.\n",
    "\n",
    "\n",
    "def align_images_feature_based(img1, img2):\n",
    "    \"\"\"\n",
    "    Align img2 to img1 using ORB feature matching + homography.\n",
    "    Handles both grayscale and color images.\n",
    "    \"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(100,100))\n",
    "    # Ensure grayscale conversion only if needed\n",
    "    if len(img1.shape) == 3:  # color\n",
    "        gray1 = clahe.apply(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY))\n",
    "    else:  # already grayscale\n",
    "        gray1 = clahe.apply(img1)\n",
    "\n",
    "    if len(img2.shape) == 3:  # color\n",
    "        gray2 = clahe.apply(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY))\n",
    "    else:  # already grayscale\n",
    "        gray2 = clahe.apply(img2)\n",
    "\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    orb = cv2.ORB_create(5000)\n",
    "    kp1, des1 = orb.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    if len(matches) < 4:\n",
    "        raise ValueError(\"Not enough matches to compute homography.\")\n",
    "\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate the homography matrix\n",
    "    M, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp img2 to align with img1 (keep same size as img1)\n",
    "    aligned = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]))\n",
    "    \n",
    "    inliers = mask.sum() if mask is not None else 0\n",
    "\n",
    "    return aligned, inliers, M\n",
    "\n",
    "\n",
    "def align_images_with_mirror_check(img1, img2):\n",
    "    best_aligned = None\n",
    "    best_inliers = -1\n",
    "\n",
    "    # Try normal alignment\n",
    "    try:\n",
    "        aligned, inliers, M = align_images_feature_based(img1, img2)\n",
    "        if inliers > best_inliers:\n",
    "            best_inliers = inliers\n",
    "            best_aligned = aligned\n",
    "            best_M = M\n",
    "    except Exception as e:\n",
    "        print(\"Normal alignment failed:\", e)\n",
    "\n",
    "    # Try mirrored alignment\n",
    "    img2_flipped = cv2.flip(img2, 1)\n",
    "    try:\n",
    "        aligned, inliers, M = align_images_feature_based(img1, img2_flipped)\n",
    "        if inliers > best_inliers:\n",
    "            best_inliers = inliers\n",
    "            best_aligned = aligned\n",
    "            best_M = M\n",
    "    except Exception as e:\n",
    "        print(\"Mirrored alignment failed:\", e)\n",
    "\n",
    "    # if best_aligned is None:\n",
    "        raise ValueError(\"Failed to align both normal and mirrored cases.\")\n",
    "\n",
    "    return best_aligned, best_M\n",
    "\n",
    "\n",
    "def overlay_images_rgb(img1, img2_aligned, alpha=0.5):\n",
    "    # Resize img2_aligned to match img1 if needed\n",
    "    if img1.shape != img2_aligned.shape:\n",
    "        img2_aligned = cv2.resize(img2_aligned, (img1.shape[1], img1.shape[0]))\n",
    "\n",
    "    # Blend images using alpha\n",
    "    blended = cv2.addWeighted(img1, alpha, img2_aligned, 1 - alpha, 0)\n",
    "    return blended\n",
    "\n",
    "\n",
    "\n",
    "# Align img2 to img1\n",
    "aligned, M = align_images_with_mirror_check(img1, img2)\n",
    "\n",
    "# Overlay for visual verification\n",
    "blended = overlay_images_rgb(img1, aligned)\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "output_folder = \"Aligned and Blended images\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Split into folder, base filename, and extension\n",
    "base2 = os.path.splitext(os.path.basename(address2))[0]   # cam0_capture_1_20250828_051554\n",
    "ext2 = os.path.splitext(address2)[1]                      # .jpg\n",
    " \n",
    "# Build new paths\n",
    "aligned_filename = os.path.join(output_folder, f\"{base2}_aligned{ext2}\")\n",
    "blended_filename = os.path.join(output_folder, f\"{base2}_blended{ext2}\")\n",
    "\n",
    "cv2.imwrite(aligned_filename, aligned)\n",
    "cv2.imwrite(blended_filename, blended)\n",
    "\n",
    "print(f\"Saved aligned image to: {aligned_filename}\")\n",
    "print(f\"Saved blended image to: {blended_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2b0a8",
   "metadata": {},
   "source": [
    "2 Frames Focus Assessment Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus loss for img1: 5.72860350544188, img2: 12.663590471384262\n",
      "The images are not in focus.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Focus loss calculation using variance of Laplacian\n",
    "def focus_loss(image):\n",
    "    # If already grayscale, skip conversion\n",
    "    if len(image.shape) == 2:  # single channel\n",
    "        gray = image\n",
    "    else:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    return variance\n",
    "\n",
    "loss1 = focus_loss(img1)\n",
    "loss2 = focus_loss(img2)\n",
    "print(f\"Focus loss for img1: {loss1}, img2: {loss2}\")\n",
    "focus_threshold = 0.5  # Example threshold for focus quality\n",
    "if np.abs(loss1 - loss2) < focus_threshold:\n",
    "    print(\"The images are in focus.\")\n",
    "else:\n",
    "    print(\"The images are not in focus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89db54f",
   "metadata": {},
   "source": [
    "Video Alignment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db556f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP4 already exists: c:\\Users\\liats\\sodium-vapor-camera-pi-5-code\\Raw Videos\\cam0_capture_20250828_063612.mp4\n",
      "MP4 already exists: c:\\Users\\liats\\sodium-vapor-camera-pi-5-code\\Raw Videos\\cam1_capture_20250828_063612.mp4\n",
      "Alignment complete. Output saved to: Aligned Videos\\cam0_capture_20250828_063612_aligned.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Helper function to convert & rotate raw H.264\n",
    "# ---------------------------\n",
    "def remux_and_rotate_h264(input_file, output_file, fps=25, rotation=None):\n",
    "    \"\"\"\n",
    "    Convert raw H.264 to MP4 with optional rotation (re-encoding required).\n",
    "    rotation: None, 90, 180, 270 (degrees clockwise)\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"MP4 already exists: {output_file}\")\n",
    "        return\n",
    "\n",
    "    cmd = [\"ffmpeg\", \"-y\", \"-framerate\", str(fps), \"-i\", input_file]\n",
    "\n",
    "    # Apply rotation with ffmpeg filters\n",
    "    if rotation == 90:\n",
    "        cmd += [\"-vf\", \"transpose=1\"]  # 90° clockwise\n",
    "    elif rotation == 180:\n",
    "        cmd += [\"-vf\", \"transpose=1,transpose=1\"]  # 180° (two 90° clockwise)\n",
    "    elif rotation == 270:\n",
    "        cmd += [\"-vf\", \"transpose=2\"]  # 90° counter-clockwise (270° clockwise)\n",
    "\n",
    "    # Re-encode with libx264 to apply rotation\n",
    "    cmd += [\"-c:v\", \"libx264\", \"-pix_fmt\", \"yuv420p\", output_file]\n",
    "\n",
    "    subprocess.run(cmd, check=True)\n",
    "    print(f\"Created MP4: {output_file}\")\n",
    "\n",
    "# ---------------------------\n",
    "# File paths\n",
    "# ---------------------------\n",
    "\n",
    "# Extract base names without .h264 extensions\n",
    "vid1_base = os.path.splitext(vid_address1)[0]  \n",
    "vid2_base = os.path.splitext(vid_address2)[0]\n",
    "\n",
    "# Future mp4 filenames\n",
    "vid1_mp4 = f\"{vid1_base}.mp4\"\n",
    "vid2_mp4 = f\"{vid2_base}.mp4\"\n",
    "\n",
    "mp4_base2 = os.path.splitext(os.path.basename(vid2_mp4))[0]\n",
    "mp4_ext2 = os.path.splitext(vid2_mp4)[1]                      # .mp4\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "output_folder = \"Aligned Videos\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "output_aligned = os.path.join(output_folder, f\"{mp4_base2}_aligned{mp4_ext2}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Convert and rotate raw H.264 to MP4\n",
    "# ---------------------------\n",
    "remux_and_rotate_h264(vid_address2, vid2_mp4, fps=25, rotation=180)\n",
    "remux_and_rotate_h264(vid_address1, vid1_mp4, fps=25, rotation=90)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Open MP4s with OpenCV\n",
    "# ---------------------------\n",
    "cap0 = cv2.VideoCapture(vid1_mp4)  # reference\n",
    "cap1 = cv2.VideoCapture(vid2_mp4)  # to align\n",
    "\n",
    "ret0, frame_ref = cap0.read()\n",
    "ret1, frame_target = cap1.read()\n",
    "if not (ret0 and ret1):\n",
    "    raise ValueError(\"Failed to read the first frames from the videos.\")\n",
    "\n",
    "h, w = frame_ref.shape[:2]\n",
    "fps = cap0.get(cv2.CAP_PROP_FPS) or 25\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_aligned, fourcc, fps, (w, h))\n",
    "\n",
    "# Reset to first frame\n",
    "cap0.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "cap1.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 3: Frame-by-frame alignment\n",
    "# ---------------------------\n",
    "\n",
    "ret0, frame_ref = cap0.read()\n",
    "ret1, frame_target = cap1.read()\n",
    "if not (ret0 and ret1):\n",
    "     raise ValueError(\"Failed to read frames for homography estimation.\")\n",
    "\n",
    "# Align cam1 to cam0 using first frame to get homography\n",
    "aligned_frame, M = align_images_with_mirror_check(frame_ref, frame_target)\n",
    "\n",
    "# Reset both videos to frame 0 again\n",
    "cap0.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "cap1.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Apply homography to all frames\n",
    "# ---------------------------\n",
    "while True:\n",
    "    ret0, frame_ref = cap0.read()\n",
    "    ret1, frame_target = cap1.read()\n",
    "    frame_target = cv2.flip(frame_target, 1)  # Flip frame_target horizontally\n",
    "    if not (ret0 and ret1):\n",
    "        break\n",
    "\n",
    "    # Apply the SAME homography to every frame of cam1\n",
    "    aligned_frame = cv2.warpPerspective(frame_target, M, (w, h))\n",
    "\n",
    "    out.write(aligned_frame)\n",
    "# ---------------------------\n",
    "# Step 5: Cleanup\n",
    "# ---------------------------\n",
    "cap0.release()\n",
    "cap1.release()\n",
    "out.release()\n",
    "\n",
    "print(\"Alignment complete. Output saved to:\", output_aligned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67eb682",
   "metadata": {},
   "source": [
    "Image Compositing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89611774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: Composited Results\\cam1_capture_1_20250828_051554_fg_only.jpg, Composited Results\\beach_bg_only_cam0_capture_1_20250828_051554_aligned.jpg, Composited Results\\cam0_capture_1_20250828_051554_aligned_matte_only.jpg, Composited Results\\cam1_capture_1_20250828_051554_beach_composited.jpg\n",
      "✅ Composite saved in 'Composited Results' folder\n"
     ]
    }
   ],
   "source": [
    "def sodium_vapor_composite_from_paths(fg, bg, matte, save,\n",
    "                                      output_folder=\"Composited Results\"):\n",
    "    \"\"\"\n",
    "    Sodium vapor compositing: combines foreground, background, and matte images.\n",
    "    Accepts either:\n",
    "        - File paths (strings)\n",
    "        - Already-loaded frames (NumPy arrays)\n",
    "    save: whether to save the output (used for still images; ignored for video frames)\n",
    "    Returns composited, fg_only, bg_only, matte, paths   \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # If input is a path, load image\n",
    "    if isinstance(fg, str):\n",
    "        foreground_color = cv2.imread(fg)\n",
    "        background_image = cv2.imread(bg)\n",
    "        sodium_matte = cv2.imread(matte, cv2.IMREAD_GRAYSCALE)\n",
    "        save_flag = save\n",
    "    else:\n",
    "        foreground_color, background_image, sodium_matte = fg, bg, matte\n",
    "        save_flag = False  # never save when passing frames\n",
    "\n",
    "    \n",
    "    # Sanity check\n",
    "    if foreground_color is None or background_image is None or sodium_matte is None:\n",
    "        raise ValueError(\"One or more image paths are invalid or files cannot be read.\")\n",
    "\n",
    "    # --- Prepare sizes ---\n",
    "    h, w = foreground_color.shape[:2]\n",
    "    if background_image.shape[:2] != (h, w):\n",
    "        background_image = cv2.resize(background_image, (w, h))\n",
    "    if sodium_matte.shape[:2] != (h, w):\n",
    "        sodium_matte = cv2.resize(sodium_matte, (w, h))\n",
    "\n",
    "    \n",
    "   # Resize bg and matte to match fg\n",
    "    background_image = cv2.resize(background_image, (foreground_color.shape[1], foreground_color.shape[0]))\n",
    "    if len(sodium_matte.shape) == 3:  # If matte is color, convert to grayscale\n",
    "        sodium_matte = cv2.cvtColor(sodium_matte, cv2.COLOR_BGR2GRAY)\n",
    "    sodium_matte = cv2.resize(sodium_matte, (foreground_color.shape[1], foreground_color.shape[0]))\n",
    "\n",
    "    # Normalize matte to 0..1 and make 3-channel\n",
    "    # matte_norm = sodium_matte.astype(\"float32\") / 255.0\n",
    "    matte_norm = cv2.normalize(sodium_matte.astype(\"float32\"), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "    matte_3ch = cv2.merge([matte_norm]*3)\n",
    "    inv_matte_3ch = 1.0 - matte_3ch\n",
    "\n",
    "    # Extract parts\n",
    "    fg_part = (foreground_color.astype(np.float32) * matte_3ch).astype(np.uint8)\n",
    "    fg_part = cv2.convertScaleAbs(fg_part, alpha=5, beta=10)  # slightly darker foreground for better blending\n",
    "\n",
    "\n",
    "    bg_part = (background_image.astype(np.float32) * inv_matte_3ch).astype(np.uint8)\n",
    "    # bg_part = cv2.convertScaleAbs(bg_part, alpha=5.5, beta=80)  # Brighten background slightly\n",
    "\n",
    "    composited = cv2.add(fg_part, bg_part)\n",
    "\n",
    "    if save: # Save results only if save is True (when compositing pics, not videos)\n",
    "        output_folder = \"Composited Results\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # Determine base names for saving only if input is a path\n",
    "        if isinstance(fg, str):\n",
    "            fg_base = os.path.splitext(os.path.basename(fg))[0]\n",
    "        else:\n",
    "            fg_base = \"fg_frame\"\n",
    "\n",
    "        if isinstance(bg, str):\n",
    "            bg_base = os.path.splitext(os.path.basename(bg))[0]\n",
    "        else:\n",
    "            bg_base = \"bg_frame\"\n",
    "        if isinstance(matte, str):\n",
    "            matte_base = os.path.splitext(os.path.basename(matte))[0]\n",
    "        else:\n",
    "            matte_base = \"matte_frame\"\n",
    "\n",
    "        fg_only_path = os.path.join(output_folder, f\"{fg_base}_fg_only.jpg\")\n",
    "        bg_only_path = os.path.join(output_folder, f\"{bg_base}_bg_only_{matte_base}.jpg\")\n",
    "        matte_only_path = os.path.join(output_folder, f\"{matte_base}_matte_only.jpg\")\n",
    "\n",
    "        composited_path = os.path.join(output_folder, f\"{fg_base}_{bg_base}_composited.jpg\")\n",
    "\n",
    "        cv2.imwrite(fg_only_path, fg_part)\n",
    "        cv2.imwrite(bg_only_path, bg_part)\n",
    "        cv2.imwrite(matte_only_path, matte_3ch*255)\n",
    "        cv2.imwrite(composited_path, composited)\n",
    "\n",
    "        print(f\"✅ Saved: {fg_only_path}, {bg_only_path}, {matte_only_path}, {composited_path}\")\n",
    "\n",
    "        return composited, fg_part, bg_part, matte_3ch, (fg_only_path, bg_only_path, matte_only_path, composited_path)\n",
    "    else:\n",
    "        return composited, fg_part, bg_part, matte_3ch, None\n",
    "\n",
    "# Load images\n",
    "fg_address = \"Raw Images/cam1_capture_1_20250828_051554.jpg\"   # actors - the \"normal\" camera image with the actors (what you want to keep).\n",
    "bg_address = \"Backgrounds/beach.jpg\"  # scenery - the replacement scenery (the plate you want to paste the actors onto).\n",
    "\n",
    "matte_address = \"Aligned and Blended images/cam0_capture_1_20250828_051554_aligned.jpg\"  # sodium channel matte - the sodium-sensitive camera image \n",
    "# (basically a mask of just the background — it looks like a silhouette of your actors in black with the sodium-lit screen in white).\n",
    "# must match the picture of the foreground_color image (but with a different camera). \n",
    "\n",
    "composited, fg_only, bg_only, matte, paths = sodium_vapor_composite_from_paths(fg_address, bg_address, matte_address, save=True)\n",
    "\n",
    "print(\"✅ Composite saved in 'Composited Results' folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992137db",
   "metadata": {},
   "source": [
    "Video Compositing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24d649",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m matte_gray = cv2.cvtColor(matte_frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Reuse compositing function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m composited, fg_only, bg_only, matte = sodium_vapor_composite_from_paths(\n\u001b[32m     66\u001b[39m     fg_frame, bg_frame, matte_gray, save=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     67\u001b[39m )\n\u001b[32m     69\u001b[39m out.write(composited)\n\u001b[32m     70\u001b[39m out_fg.write(fg_only)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(\"Composited Results\", exist_ok=True)\n",
    "\n",
    "# Inputs\n",
    "fg_cap = cv2.VideoCapture(\"Raw Videos/cam1_capture_20250828_063612.mp4\")\n",
    "bg_path = \"Backgrounds/Zisapel-Building.jpg\"   # <- can be video OR image\n",
    "bg_base = os.path.splitext(os.path.basename(bg_path))[0]\n",
    "\n",
    "matte_cap = cv2.VideoCapture(\"Aligned Videos/cam0_capture_20250828_063612_aligned.mp4\")\n",
    "\n",
    "bg_cap = cv2.VideoCapture(bg_path)\n",
    "\n",
    "# How many frames does it report?\n",
    "frame_count = int(bg_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "if frame_count <= 1:\n",
    "    # Treat as still image\n",
    "    ret, frame = bg_cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(f\"Could not read background: {bg_path}\")\n",
    "    bg_image = frame\n",
    "    bg_cap.release()\n",
    "    bg_cap = None\n",
    "    bg_is_video = False\n",
    "else:\n",
    "    bg_image = None\n",
    "    bg_is_video = True\n",
    "\n",
    "\n",
    "# Setup writer\n",
    "fps = fg_cap.get(cv2.CAP_PROP_FPS)\n",
    "w = int(fg_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(fg_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "composited_path = os.path.join(\"Composited Results\", f\"final_composite_{bg_base}.mp4\")\n",
    "foreground_path = os.path.join(\"Composited Results\", f\"final_foreground_{bg_base}.mp4\")\n",
    "background_path = os.path.join(\"Composited Results\", f\"final_background_{bg_base}.mp4\")\n",
    "matte_path = os.path.join(\"Composited Results\", f\"final_matte_{bg_base}.mp4\")\n",
    "\n",
    "out = cv2.VideoWriter(composited_path, fourcc, fps, (w, h))\n",
    "out_fg = cv2.VideoWriter(foreground_path, fourcc, fps, (w, h))\n",
    "out_bg = cv2.VideoWriter(background_path, fourcc, fps, (w, h))\n",
    "out_matte = cv2.VideoWriter(matte_path, fourcc, fps, (w, h))\n",
    "\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret_fg, fg_frame = fg_cap.read()\n",
    "    ret_matte, matte_frame = matte_cap.read()\n",
    "\n",
    "    if not (ret_fg and ret_matte):\n",
    "        break\n",
    "\n",
    "    # Handle background\n",
    "    if bg_is_video:\n",
    "        ret_bg, bg_frame = bg_cap.read()\n",
    "        if not ret_bg:\n",
    "            break\n",
    "    else:\n",
    "        bg_frame = cv2.resize(bg_image, (w, h))  # reuse same still image every frame\n",
    "\n",
    "    # Convert matte to grayscale\n",
    "    matte_gray = cv2.cvtColor(matte_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Reuse compositing function\n",
    "    composited, fg_only, bg_only, matte, paths = sodium_vapor_composite_from_paths(\n",
    "        fg_frame, bg_frame, matte_gray, save=False\n",
    "    )\n",
    "\n",
    "    out.write(composited)\n",
    "    out_fg.write(fg_only)\n",
    "    out_bg.write(bg_only)\n",
    "    out_matte.write((matte*255).astype(np.uint8))\n",
    "    frame_idx += 1\n",
    "    if frame_idx % 50 == 0:\n",
    "        print(f\"Processed {frame_idx} frames...\")\n",
    "\n",
    "fg_cap.release()\n",
    "if bg_is_video:\n",
    "    bg_cap.release()\n",
    "matte_cap.release()\n",
    "out.release()\n",
    "out_fg.release()\n",
    "out_bg.release()\n",
    "out_matte.release()\n",
    "\n",
    "print(\"✅ Video compositing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
