{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe588324",
   "metadata": {},
   "source": [
    "Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d415539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc02ba8",
   "metadata": {},
   "source": [
    "2 Frames Alignment Function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c39c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script aligns two images using feature-based methods.\n",
    "# It uses ORB feature detection and matching to compute a homography matrix,   \n",
    "# which is then used to warp one image to align with the other.\n",
    "# The aligned image is then tested against the original image to ensure alignment.\n",
    "img1 = cv2.imread('cam0_me.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('cam1_me.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "def align_images_feature_based(img1, img2):\n",
    "    # Convert both to grayscale\n",
    "    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect ORB keypoints and descriptors\n",
    "    orb = cv2.ORB_create(5000)\n",
    "    kp1, des1 = orb.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "    # Match features using BFMatcher\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    if len(matches) < 4:\n",
    "        raise ValueError(\"Not enough matches to compute homography.\")\n",
    "\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Extract location of good matches\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate the homography matrix\n",
    "    M, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp img2 to align with img1\n",
    "    aligned = cv2.warpPerspective(img2, M, (img1.shape[1], img1.shape[0]))\n",
    "    return aligned\n",
    "\n",
    "def overlay_images_rgb(img1, img2_aligned, alpha=0.5):\n",
    "    # Resize img2_aligned to match img1 if needed\n",
    "    if img1.shape != img2_aligned.shape:\n",
    "        img2_aligned = cv2.resize(img2_aligned, (img1.shape[1], img1.shape[0]))\n",
    "\n",
    "    # Blend images using alpha\n",
    "    blended = cv2.addWeighted(img1, alpha, img2_aligned, 1 - alpha, 0)\n",
    "    return blended\n",
    "\n",
    "\n",
    "# Load original and rotated images\n",
    "img1 = cv2.imread(\"cam0_me.jpg\")\n",
    "img2 = cv2.imread(\"cam1_me.jpg\")\n",
    "\n",
    "# Align img2 to img1\n",
    "aligned = align_images_feature_based(img1, img2)\n",
    "\n",
    "# Overlay for visual verification\n",
    "blended = overlay_images_rgb(img1, aligned)\n",
    "\n",
    "# Save and show\n",
    "cv2.imwrite(\"aligned_output_me_lowlight.jpg\", aligned)\n",
    "cv2.imwrite(\"overlay_check_me_lowlight.jpg\", blended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2b0a8",
   "metadata": {},
   "source": [
    "2 Frames Focus Assessment Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21da5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus loss for img1: -77.45220696343317, img2: -70.7470047929128\n",
      "The images are not in focus.\n"
     ]
    }
   ],
   "source": [
    "def focus_loss(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    variance = laplacian.var()\n",
    "    # Loss is high when image is blurry (i.e., variance is low)\n",
    "    return -variance\n",
    "loss1 = focus_loss(img1)\n",
    "loss2 = focus_loss(img2)\n",
    "print(f\"Focus loss for img1: {loss1}, img2: {loss2}\")\n",
    "focus_threshold = 0.5  # Example threshold for focus quality\n",
    "if np.abs(loss1 - loss2) < focus_threshold:\n",
    "    print(\"The images are in focus.\")\n",
    "else:\n",
    "    print(\"The images are not in focus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89db54f",
   "metadata": {},
   "source": [
    "Video Alignment Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db556f30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to read the first frames.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m ret2, frame_target = cap2.read()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (ret1 \u001b[38;5;129;01mand\u001b[39;00m ret2):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mFailed to read the first frames.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Get video properties\u001b[39;00m\n\u001b[32m     12\u001b[39m h, w = frame_ref.shape[:\u001b[32m2\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Failed to read the first frames."
     ]
    }
   ],
   "source": [
    "## The Pi 5 records in video format, not in mp4. This needs to be fixed. \n",
    "# Open both videos\n",
    "cap1 = cv2.VideoCapture(\"video1.mp4\")  # reference\n",
    "cap2 = cv2.VideoCapture(\"video2.mp4\")  # to align\n",
    "\n",
    "# Read one frame to get size/fps info\n",
    "ret1, frame_ref = cap1.read()\n",
    "ret2, frame_target = cap2.read()\n",
    "if not (ret1 and ret2):\n",
    "    raise ValueError(\"Failed to read the first frames.\")\n",
    "\n",
    "# Get video properties\n",
    "h, w = frame_ref.shape[:2]\n",
    "fps = cap1.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"video2_aligned.mp4\", fourcc, fps, (w, h))\n",
    "\n",
    "# Reset videos to the start\n",
    "cap1.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "cap2.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "# Frame-by-frame alignment\n",
    "while True:\n",
    "    ret1, frame_ref = cap1.read()\n",
    "    ret2, frame_target = cap2.read()\n",
    "    if not (ret1 and ret2):\n",
    "        break\n",
    "\n",
    "    aligned_frame = align_images_feature_based(frame_ref, frame_target)\n",
    "    out.write(aligned_frame)\n",
    "\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "out.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
